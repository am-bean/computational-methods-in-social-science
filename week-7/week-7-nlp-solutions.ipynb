{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "80ac79ca-a0ef-4081-a14b-42ec81ce898f",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": null
    },
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "# Week 7: Cleaning (Text) Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "d9c467b0-446d-4028-aa67-d2feb87132ca",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 18
    },
    "deepnote_cell_height": 226,
    "deepnote_cell_type": "markdown",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "In this notebook we use the `nltk` python library to perform basic natural language processing tasks. NLTK stands for `natural language toolkit`, and can be used for a lot of standard tasks related to cleaning and analysing text data.\n",
    "\n",
    "We will be using the dataset that we worked with last week, the combined comment sections of two Reddit posts. Although you are encouraged to use the files that you made last week, I have also provided the pre-made files to ensure everyone is able to start in the same place.\n",
    "\n",
    "This week, we will count word frequencies, which will lead to observations about `stop words`, and `lemmatisation`. We will use the word frequencies to compare the two different comment sections, and use a pre-trained sentiment analysis tool to estimate the sentiments of the comments. The homework will contain an optional exercise using word frequencies and a `Naive Bayes` classifier to guess which comment section a particular comment came from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cell_id": "5efbed6a-e825-4b2a-8bda-7cc4f2671139",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 24
    },
    "deepnote_cell_height": 99,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 3020,
    "execution_start": 1646038770920,
    "source_hash": "a933caac",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Run this cell to install the nltk library using the Python package manager pip\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cell_id": "c52acc9e-91b3-49d7-b54a-a85bb87fd2c7",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 30
    },
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 0,
    "execution_start": 1646038773946,
    "source_hash": "f6cb1ae6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load your data files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**Exercise 0.1:** Read the CSV files from last week into two dataframes.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas does all the work for us\n",
    "\n",
    "mammoths_df = pd.read_csv('mammoths.csv')\n",
    "remote_df = pd.read_csv('remote_work.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>created_at</th>\n",
       "      <th>id</th>\n",
       "      <th>parent</th>\n",
       "      <th>score</th>\n",
       "      <th>post</th>\n",
       "      <th>has_emoji</th>\n",
       "      <th>clean_body</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>when you say it like that - gimme 2</td>\n",
       "      <td>2023-02-01 07:42:00</td>\n",
       "      <td>j6r1s1s</td>\n",
       "      <td>t1_j6qxv18</td>\n",
       "      <td>6</td>\n",
       "      <td>10py29t</td>\n",
       "      <td>False</td>\n",
       "      <td>['when', 'you', 'say', 'it', 'like', 'that', '...</td>\n",
       "      <td>0.3612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mammork?\\n\\nI would that make them hammoths?</td>\n",
       "      <td>2023-02-01 10:18:59</td>\n",
       "      <td>j6rd2o8</td>\n",
       "      <td>t1_j6qxv18</td>\n",
       "      <td>3</td>\n",
       "      <td>10py29t</td>\n",
       "      <td>False</td>\n",
       "      <td>['mammork', 'i', 'would', 'that', 'make', 'the...</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mmmmmmmm mammoth pork</td>\n",
       "      <td>2023-02-01 06:52:37</td>\n",
       "      <td>j6qxv18</td>\n",
       "      <td>t1_j6obhxv</td>\n",
       "      <td>22</td>\n",
       "      <td>10py29t</td>\n",
       "      <td>False</td>\n",
       "      <td>['mmmmmmmm', 'mammoth', 'pork']</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It’ll just be made from pressed pork.</td>\n",
       "      <td>2023-01-31 19:16:08</td>\n",
       "      <td>j6obhxv</td>\n",
       "      <td>t1_j6notr0</td>\n",
       "      <td>68</td>\n",
       "      <td>10py29t</td>\n",
       "      <td>False</td>\n",
       "      <td>['itll', 'just', 'be', 'made', 'from', 'presse...</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11 years before they’re extinct again</td>\n",
       "      <td>2023-02-01 06:51:58</td>\n",
       "      <td>j6qxt2e</td>\n",
       "      <td>t1_j6notr0</td>\n",
       "      <td>7</td>\n",
       "      <td>10py29t</td>\n",
       "      <td>False</td>\n",
       "      <td>['11', 'years', 'before', 'theyre', 'extinct',...</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           body           created_at       id  \\\n",
       "0           when you say it like that - gimme 2  2023-02-01 07:42:00  j6r1s1s   \n",
       "1  Mammork?\\n\\nI would that make them hammoths?  2023-02-01 10:18:59  j6rd2o8   \n",
       "2                         Mmmmmmmm mammoth pork  2023-02-01 06:52:37  j6qxv18   \n",
       "3         It’ll just be made from pressed pork.  2023-01-31 19:16:08  j6obhxv   \n",
       "4         11 years before they’re extinct again  2023-02-01 06:51:58  j6qxt2e   \n",
       "\n",
       "       parent  score     post  has_emoji  \\\n",
       "0  t1_j6qxv18      6  10py29t      False   \n",
       "1  t1_j6qxv18      3  10py29t      False   \n",
       "2  t1_j6obhxv     22  10py29t      False   \n",
       "3  t1_j6notr0     68  10py29t      False   \n",
       "4  t1_j6notr0      7  10py29t      False   \n",
       "\n",
       "                                          clean_body  sentiment  \n",
       "0  ['when', 'you', 'say', 'it', 'like', 'that', '...     0.3612  \n",
       "1  ['mammork', 'i', 'would', 'that', 'make', 'the...     0.0000  \n",
       "2                    ['mmmmmmmm', 'mammoth', 'pork']     0.0000  \n",
       "3  ['itll', 'just', 'be', 'made', 'from', 'presse...     0.0000  \n",
       "4  ['11', 'years', 'before', 'theyre', 'extinct',...     0.0000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking that we got the same thing as before\n",
    "mammoths_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Data\n",
    "\n",
    "Text data is often very messy to work with. There can be a lot of annoying $pecial characters, spelling misteaks, Capitalisation choices, and artefacts of the source material like newlines `\\n`. We will take a simple approach to cleaning - remove anything that isn't alphanumeric and make everything lowercase. You may be more careful about this approach if the research question calls for it. For example, keeping track of sentence and paragraph breaks is often helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to split up a string between the spaces:\n",
    "example_string = 'The quick brown fox jumped over the lazy dog            '\n",
    "example_string.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the quick brown fox jumped over the lazy dog            '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to make a string lowercase\n",
    "example_string.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The quick brown fox jumped over the lazy dog'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to remove extra whitespace around a string\n",
    "example_string.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all together\n",
    "example_string.lower().strip().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we use another nice property of pandas!\n",
    "# if you write a function, you can apply it to every entry in a column at once!\n",
    "\n",
    "# our basic cleaning function\n",
    "def clean_text(text):\n",
    "    return [re.sub('[^a-z0-9]', '', w.lower()) for w in text.strip().split()]\n",
    "\n",
    "# now we apply it all at once using pandas\n",
    "mammoths_df['clean_body'] = mammoths_df['body'].apply(clean_text)\n",
    "remote_df['clean_body'] = remote_df['body'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "body             Mammork?\\n\\nI would that make them hammoths?\n",
       "created_at                                2023-02-01 10:18:59\n",
       "id                                                    j6rd2o8\n",
       "parent                                             t1_j6qxv18\n",
       "score                                                       3\n",
       "post                                                  10py29t\n",
       "has_emoji                                               False\n",
       "clean_body    [mammork, i, would, that, make, them, hammoths]\n",
       "sentiment                                                 0.0\n",
       "Name: 1, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's look at one example:\n",
    "# We've removed the two \\n characters and the punctuation, and split the words into a list\n",
    "# The vocab is made up which will make things more difficult later\n",
    "\n",
    "mammoths_df.iloc[1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "body          I was able to leverage my experience and the l...\n",
       "created_at                                  2023-01-31 23:12:07\n",
       "id                                                      j6pd2pm\n",
       "parent                                               t1_j6p4jgg\n",
       "score                                                         2\n",
       "post                                                    10plq1f\n",
       "has_emoji                                                 False\n",
       "clean_body    [i, was, able, to, leverage, my, experience, a...\n",
       "sentiment                                                0.8756\n",
       "Name: 1, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For a more standard text this seems more reasonable\n",
    "remote_df.iloc[1,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Frequencies\n",
    "\n",
    "Computational linguists have found a number of surprising regularities in language which can be used in computational social science. For example, the frequency with which words are used tends to be exponentially distributed, so that the most frequent words are **far** more common than the next most frequent words.This also holds true within subject areas, where some vocabulary words are extremely common within their area of use, but they are often very uncommon in general language. This can be used to identify the topic of a body of text based on the frequent words within it.\n",
    "\n",
    "As an example, if you overheard the words `formal hall`, `college`, and `tutorial` you might guess that person is talking about Oxford. The words are rare in general usage, but in a specific context they are very standard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**Exercise 0.2:** Count the most common words in each dataframe using a Counter\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'the': 2,\n",
       "         'quick': 1,\n",
       "         'brown': 1,\n",
       "         'fox': 1,\n",
       "         'jumped': 1,\n",
       "         'over': 1,\n",
       "         'lazy': 1,\n",
       "         'dog': 1})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A Counter goes through the items in a list or dictionary and tells you how many there are of each\n",
    "txt_example = ['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog']\n",
    "\n",
    "Counter(txt_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need two for loops to get the words one at a time rather than the comments one at a time\n",
    "\n",
    "counts = Counter(word for text in mammoths_df['clean_body'] for word in text)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  word  count\n",
      "0  the    800\n",
      "1   to    566\n",
      "2    a    503\n",
      "3  and    382\n",
      "4   of    350\n",
      "         word  count\n",
      "3489   seemed      1\n",
      "3490     star      1\n",
      "3491     wars      1\n",
      "3492  execute      1\n",
      "3493       66      1\n"
     ]
    }
   ],
   "source": [
    "# Counters have a most_common() method which tells us which words we saw the most\n",
    "\n",
    "# Let's put it in a dataframe and look at the top and bottom few\n",
    "\n",
    "mammoth_counts_df = pd.DataFrame(counts.most_common(), columns = ['word','count'])\n",
    "\n",
    "print(mammoth_counts_df.head())\n",
    "\n",
    "print(mammoth_counts_df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**Exercise 0.2b:** We'll learn how to make plots next week. For now, I'll show you a plot of the frequencies of the words in this comment section\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Note that this plot is logarithmic on both axes\n",
    "# This tells us that (as expected) a small number of words make up the vast majority of the text\n",
    "\n",
    "plt.loglog(mammoth_counts_df.index, mammoth_counts_df['count'])\n",
    "plt.title('Frequency vs Frequency Rank')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['the', 'to', 'a', 'and', 'of', 'in', 'it', 'that', 'i', 'they',\n",
       "       'is', 'this', 'mammoth', 'for', 'we', 'be', 'are', 'but', 'not',\n",
       "       'with', 'have', 'its', 'years', 'you', 'just', 'like', 'on', 'if',\n",
       "       'back', 'so', 'mammoths', 'about', 'an', '', 'can', 'from', 'was',\n",
       "       'do', 'will', 'as', 'would', 'or', 'why', 'what', 'them', 'dna',\n",
       "       'were', 'there', 'their', 'how', 'could', 'extinct', 'all', 'when',\n",
       "       'time', 'elephant', 'think', 'im', 'elephants', 'one', 'woolly',\n",
       "       'even', 'at', 'get', 'some', 'dont', 'more', 'species', 'jurassic',\n",
       "       'going', 'see', 'park', 'up', 'now', 'been', 'by', 'people',\n",
       "       'bring', 'out', 'go', 'into', 'no', 'know', 'then', 'well', 'make',\n",
       "       'theyre'], dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In fact, just these 87 words make up half the total words used even though there are 3494 total unique words\n",
    "\n",
    "mammoth_counts_df[mammoth_counts_df['count'].cumsum() < (mammoth_counts_df['count'].sum()/2)]['word'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "210db550-fe4d-4e4f-a250-da92c07137d1",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 78
    },
    "deepnote_cell_height": 229.8000030517578,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**Exercise 1.1:** Use the methods we have just demonstrated to count the word frequencies in the `remote_df`\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need two for loops to get the words one at a time rather than the comments one at a time\n",
    "\n",
    "counts = Counter(word for text in remote_df['clean_body'] for word in text)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  word  count\n",
      "0  the   1132\n",
      "1   to   1108\n",
      "2    i   1052\n",
      "3  and    847\n",
      "4    a    747\n",
      "            word  count\n",
      "3966   dedicated      1\n",
      "3967         800      1\n",
      "3968  midmorning      1\n",
      "3969    browsing      1\n",
      "3970   debugging      1\n"
     ]
    }
   ],
   "source": [
    "# Counters have a most_common() method which tells us which words we saw the most\n",
    "\n",
    "# Let's put it in a dataframe and look at the top and bottom few\n",
    "\n",
    "remote_counts_df = pd.DataFrame(counts.most_common(), columns = ['word','count'])\n",
    "\n",
    "print(remote_counts_df.head())\n",
    "\n",
    "print(remote_counts_df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Just like for the mammoths, this relationship is logarithmic\n",
    "\n",
    "plt.loglog(remote_counts_df.index, remote_counts_df['count'])\n",
    "plt.title('Frequency vs Frequency Rank')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['the', 'to', 'i', 'and', 'a', 'work', 'of', 'in', 'my', 'that',\n",
       "       'is', 'for', 'it', 'you', 'at', 'time', 'more', 'not', 'have',\n",
       "       'home', 'from', 'office', 'this', 'on', 'working', 'are', 'but',\n",
       "       'get', 'with', 'hours', 'so', 'people', 'me', 'be', 'im', 'its',\n",
       "       'or', 'if', 'dont', 'day', 'do', 'they', 'like', 'can', 'all',\n",
       "       'up', 'was', 'just', 'as', 'an', '', 'commute', 'back', 'out',\n",
       "       'when', 'we', 'because', 'no', 'your', 'wfh', 'about', 'some',\n",
       "       'now'], dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This time just these 64 words make up half the total words used even though there are 3971 total unique words\n",
    "\n",
    "remote_counts_df[remote_counts_df['count'].cumsum() < (remote_counts_df['count'].sum()/2)]['word'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>Exercise 1.2:</b> Compare the two lists of top words. How easy are they to tell apart?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "cell_id": "9196af2b-8eb8-4320-96c3-60ac9607938a",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 90
    },
    "deepnote_cell_height": 111.69999694824219,
    "deepnote_cell_type": "code",
    "deepnote_output_heights": [
     21.1875
    ],
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 3,
    "execution_start": 1645985832700,
    "source_hash": "efa21cb7",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['the', 'to', 'i', 'and', 'a', 'work', 'of', 'in', 'my', 'that',\n",
       "       'is', 'for', 'it', 'you', 'at', 'time', 'more', 'not', 'have',\n",
       "       'home', 'from', 'office', 'this', 'on', 'working', 'are', 'but',\n",
       "       'get', 'with', 'hours', 'so', 'people', 'me', 'be', 'im', 'its',\n",
       "       'or', 'if', 'dont', 'day', 'do', 'they', 'like', 'can', 'all',\n",
       "       'up', 'was', 'just', 'as', 'an', '', 'commute', 'back', 'out',\n",
       "       'when', 'we', 'because', 'no', 'your', 'wfh', 'about', 'some',\n",
       "       'now'], dtype=object)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remote work\n",
    "remote_counts_df[remote_counts_df['count'].cumsum() < (remote_counts_df['count'].sum()/2)]['word'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['the', 'to', 'a', 'and', 'of', 'in', 'it', 'that', 'i', 'they',\n",
       "       'is', 'this', 'mammoth', 'for', 'we', 'be', 'are', 'but', 'not',\n",
       "       'with', 'have', 'its', 'years', 'you', 'just', 'like', 'on', 'if',\n",
       "       'back', 'so', 'mammoths', 'about', 'an', '', 'can', 'from', 'was',\n",
       "       'do', 'will', 'as', 'would', 'or', 'why', 'what', 'them', 'dna',\n",
       "       'were', 'there', 'their', 'how', 'could', 'extinct', 'all', 'when',\n",
       "       'time', 'elephant', 'think', 'im', 'elephants', 'one', 'woolly',\n",
       "       'even', 'at', 'get', 'some', 'dont', 'more', 'species', 'jurassic',\n",
       "       'going', 'see', 'park', 'up', 'now', 'been', 'by', 'people',\n",
       "       'bring', 'out', 'go', 'into', 'no', 'know', 'then', 'well', 'make',\n",
       "       'theyre'], dtype=object)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mammoths \n",
    "\n",
    "mammoth_counts_df[mammoth_counts_df['count'].cumsum() < (mammoth_counts_df['count'].sum()/2)]['word'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of these words are very similar in both lists. Of those, nearly all of them are words you would find **everywhere** in the English language. Words that are too frequent are not very useful for analysing the text, because you would find them in any text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Words\n",
    "\n",
    "We have counted up all the words in each comment section and found that the top words are very similar in each case even though the topics are very different. To help deal with words that are too common, it is typical to remove a standard list of common words, called `stop words`. Most NLP packages will have their own concept of stop words to remove. You may also want to customise the stop words based on your context. We will start with the standard stop words from NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Andre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# here we download the stopwords used in NLTK\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "print(stopwords.words('english'))\n",
    "\n",
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>Exercise 2.1:</b> Recreate the Counters for each post, but this time don't include stop words\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['mammoth', 'years', 'like', 'back', 'mammoths', '', 'would', 'dna',\n",
       "       'could', 'extinct', 'time', 'elephant', 'think', 'im', 'elephants',\n",
       "       'one', 'woolly', 'even', 'get', 'dont', 'species', 'jurassic',\n",
       "       'going', 'see', 'park', 'people', 'bring', 'go', 'know', 'well',\n",
       "       'make', 'theyre', 'thats', 'cant', 'already', 'still', 'humans',\n",
       "       'animals', 'way', 'wooly', 'animal', 'extinction', 'warming',\n",
       "       'scientists', 'actually', 'didnt', 'really', 'enough', 'also',\n",
       "       'need', 'thing', 'colossal', 'great', 'probably', 'global', 'much',\n",
       "       'new', 'read', 'long', 'change', 'climate', 'good', 'human', 'ice',\n",
       "       'remember', 'meat', 'ago', 'life', 'something', 'saying', 'doesnt',\n",
       "       'ive', 'right', 'point', 'stop', 'getting', 'sure', 'wrong',\n",
       "       'idea', 'man', 'many', '5', 'take', 'world', 'us', 'help',\n",
       "       'science', 'want', 'around', '4', 'believe', 'cloning', 'arctic',\n",
       "       'next', 'company', 'say', 'big', 'things', 'yeah', 'last', 'clone',\n",
       "       'hope', '2', 'wait', 'mean', '10', 'eat', 'every', 'trex', 'isnt',\n",
       "       'use', 'asian', 'theyve', 'bringing', 'fuck', 'keep', 'genetic',\n",
       "       'made', 'youre', 'year', 'two', 'pretty', 'permafrost', 'first',\n",
       "       'arent', 'said', 'find', 'nothing', 'age', 'guy', 'reincarnating',\n",
       "       'dinosaurs', 'ecosystem', 'living', 'hard', 'frozen', 'ivory',\n",
       "       'put', 'real', 'bad', 'look', 'movie', 'lot', 'work', 'survive',\n",
       "       'problem', 'since', 'ill', 'got', 'another', 'seen', 'away',\n",
       "       'better', 'cool', 'news', 'theyll', 'live', 'deextinction',\n",
       "       'watch', 'finally', 'fucking', 'whether', 'maybe', 'yes',\n",
       "       'process', 'habitat', 'kill', 'id', 'wont', 'able', 'shit', '3',\n",
       "       'ever', 'god', 'thought', 'making', 'type', 'gene', 'editing',\n",
       "       'heard', 'trees', 'similar', 'earth', 'play', 'theres', 'end',\n",
       "       'found', 'times', 'used', 'money', 'fact', 'brought', 'trying',\n",
       "       'reviving', 'anyone', 'imagine', 'room', 'embryo', 'north',\n",
       "       'environment', 'different', 'anything', 'working', 'planet',\n",
       "       'please', 'instead', 'basically', 'interesting', 'might', 'due',\n",
       "       'without', 'reason', '1', 'easy', 'reading', 'giant', 'bunch',\n",
       "       'sounds', 'die', 'dodo', 'happen', 'birth', 'almost', 'went',\n",
       "       'using', 'create', 'melting', 'day', 'give', 'creature', 'talking',\n",
       "       'says', 'african', 'single', 'seems', 'part', 'done', 'release',\n",
       "       'raptors', 'current', 'preoccupied', 'itll', 'start', 'stuff',\n",
       "       'period', 'wild', 'wouldnt', 'cause', 'interglacial', 'thousands',\n",
       "       'literally', 'true', 'currently', 'tusks', 'lets', 'apparently',\n",
       "       'oh', 'sorry', 'gonna', 'gone', 'article', 'likely', 'case',\n",
       "       'dolly', 'issues', 'genes', 'hearing', 'coming', 'weve', 'less',\n",
       "       'siberia', 'scientist', 'wonder', 'worry', 'whole', 'somehow',\n",
       "       'funding', 'breeding', 'successful', 'means', 'lol', 'goal',\n",
       "       'crispr', 'existing'], dtype=object)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This time the list of words is more unique to the subject area\n",
    "# It also takes more words to capture 50% of the total usage\n",
    "\n",
    "counts = Counter(word for text in mammoths_df['clean_body'] for word in text if word not in stopwords)\n",
    "mammoth_counts_df = pd.DataFrame(counts.most_common(), columns = ['word','count'])\n",
    "\n",
    "mammoth_counts_df[mammoth_counts_df['count'].cumsum() < (mammoth_counts_df['count'].sum()/2)]['word'].values\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['work', 'time', 'home', 'office', 'working', 'get', 'hours',\n",
       "       'people', 'im', 'dont', 'day', 'like', '', 'commute', 'back',\n",
       "       'wfh', 'much', 'also', 'remote', 'go', 'even', 'would', 'less',\n",
       "       'job', 'hour', 'commuting', 'done', 'want', 'getting', 'business',\n",
       "       'way', 'life', '2', 'every', 'take', 'need', 'one', 'lunch',\n",
       "       'better', 'ive', 'week', 'thats', 'article', 'going', 'still',\n",
       "       'spend', 'make', 'extra', 'days', 'really', 'us', 'actually',\n",
       "       'money', 'traffic', 'company', 'never', 'minutes', 'insider',\n",
       "       'lot', 'think', 'know', 'see', 'id', 'around', 'shit', 'big',\n",
       "       'able', 'since', 'saved', 'well', 'things', 'instead', 'drive',\n",
       "       'meetings', 'could', 'youre', '5', 'fuck', 'long', 'good', 'lol',\n",
       "       'everyone', 'workers', 'yeah', 'doesnt', 'isnt', 'bullshit',\n",
       "       'something', 'maybe', 'got', 'away', 'theres', 'many', 'boss',\n",
       "       'remotely', 'bad', 'driving', 'jobs', 'gas', 'years', 'theyre',\n",
       "       'cant', 'sure', 'car', 'walk', 'always', 'put', 'care', 'paid',\n",
       "       'saving', 'companies', 'point', 'worked', 'etc', 'free', 'nice',\n",
       "       'pay', 'morning', 'fucking', 'end', 'great', 'used', 'two', 'per',\n",
       "       'anything', 'save', 'trying', 'longer', 'stuff', 'try', 'thing',\n",
       "       'house', 'real', 'true', 'didnt', 'live', 'person', 'hate',\n",
       "       'coffee', 'productive', 'desk', 'social', 'say', 'love', 'ill',\n",
       "       'makes', 'right', 'making', '15', 'come', 'might', 'saves',\n",
       "       'exactly', 'sometimes', 'feel', 'ever', 'eat', 'means',\n",
       "       'employees', 'use', 'family', 'articles', '3', 'break', 'amount',\n",
       "       '10', 'probably', 'new', 'actual', 'enough', 'space', 'nothing',\n",
       "       'corporate', 'wake', 'bed', 'rather', 'little', 'bit', 'look',\n",
       "       'start', 'year', 'different', 'kids', 'though', 'keep', 'sleep',\n",
       "       'leave', 'ready', '8', 'spent', 'later', 'without', 'told',\n",
       "       'laundry', 'coworkers'], dtype=object)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This time the list of words is more unique to the subject area\n",
    "# It also takes more words to capture 50% of the total usage\n",
    "\n",
    "counts = Counter(word for text in remote_df['clean_body'] for word in text if word not in stopwords)\n",
    "remote_counts_df = pd.DataFrame(counts.most_common(), columns = ['word','count'])\n",
    "\n",
    "remote_counts_df[remote_counts_df['count'].cumsum() < (remote_counts_df['count'].sum()/2)]['word'].values\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might still notice a problem here. What about words like `mammoth` and `mammoths`? Should we really be counting them separately? What about `work` and `working`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming and Lemmatisation\n",
    "\n",
    "The process of combining words of the same root into a single `token` is called either `stemming` or `lemmatisation`. Stemming is the more basic approach. You chop off the end of the word to get a shared set of letters which should be in common across different forms. This seems intellectually suspicious, but works pretty well, with our examples both being resolved: `mammoths` becomes `mammoth` and `working` becomes `work`. Lemmatisation is somewhat more complicated, and uses a dictionary to find root words. For mammoths, the result is the same, but for a word like `working`, you might lemmatise into `working` to avoid the ambiguity of the multiple meanings of `work`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert-info alert\">\n",
    "\n",
    "**Example 3.1:** Use the NLTK lemmatizer on a few example words:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Andre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Andre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mammoth',\n",
       " 'do',\n",
       " 'not',\n",
       " 'like',\n",
       " 'working',\n",
       " 'remotely',\n",
       " 'because',\n",
       " 'they',\n",
       " 'prefer',\n",
       " 'social',\n",
       " 'activity']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print out the result of lemmatizing each word in this phase:\n",
    "# this doesn't seem perfect\n",
    "\n",
    "example_words = 'mammoths do not like working remotely because they prefer social activities'.split()\n",
    "\n",
    "[lemmatizer.lemmatize(word) for word in example_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert-info alert\">\n",
    "\n",
    "**Example 3.2:** Use the NLTK stemmer on a few example words:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mammoth',\n",
       " 'do',\n",
       " 'not',\n",
       " 'like',\n",
       " 'work',\n",
       " 'remot',\n",
       " 'becaus',\n",
       " 'they',\n",
       " 'prefer',\n",
       " 'social',\n",
       " 'activ']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using the same list as before, but stemming\n",
    "\n",
    "example_words = 'mammoths do not like working remotely because they prefer social activities'\n",
    "\n",
    "[stemmer.stem(word) for word in example_words.split()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert-info alert\">\n",
    "\n",
    "**Example 3.3:** Use the NLTK lemmatizer on the reddit posts and then retry the word counting (exclude stop words)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['mammoth', 'year', 'elephant', 'like', 'back', '', 'would', 'time',\n",
       "       'dna', 'animal', 'could', 'extinct', 'think', 'im', 'human', 'one',\n",
       "       'get', 'woolly', 'even', 'dont', 'specie', 'jurassic', 'going',\n",
       "       'see', 'park', 'go', 'people', 'thing', 'bring', 'make', 'know',\n",
       "       'well', 'theyre', 'thats', 'cant', 'scientist', 'already', 'still',\n",
       "       'way', 'need', 'extinction', 'wooly', 'warming', 'change',\n",
       "       'actually', 'didnt', 'really', 'great', 'enough', 'say', 'also',\n",
       "       'colossal', 'climate', 'mean', 'probably', 'global', 'much', 'new',\n",
       "       'read', 'long', 'right', 'good', 'ice', 'remember', 'life', 'want',\n",
       "       'take', 'meat', 'point', 'stop', 'ago', 'something', 'saying',\n",
       "       'doesnt', 'ive', 'getting', 'sure', 'world', 'wrong', 'hope',\n",
       "       'idea', 'help', 'science', 'man', 'many', '5', 'clone', 'u',\n",
       "       'around', '4', 'last', 'believe', 'cloning', 'gene', 'find',\n",
       "       'arctic', 'next', 'movie', 'ecosystem', 'company', 'big', 'yeah',\n",
       "       'work', 'problem', 'guy', 'look', '2', 'wait', '10', 'eat',\n",
       "       'every', 'asian', 'keep', 'trex', 'isnt', 'use', 'process',\n",
       "       'theyve', 'bringing', 'fact', 'fuck', 'genetic', 'dinosaur',\n",
       "       'made', 'youre', 'two', 'pretty', 'lot', 'permafrost', 'age',\n",
       "       'first', 'arent', 'let', 'dodo', 'said', 'nothing', 'article',\n",
       "       'come', 'creature', 'reincarnating', 'living', 'population',\n",
       "       'habitat', 'kill', 'day', 'hard', 'frozen', 'ivory', 'put', 'real',\n",
       "       'thought', 'god', 'bad', 'environment', 'planet', 'yes', 'baby',\n",
       "       'period', 'play', 'survive', 'reason', 'end', 'since', 'ill',\n",
       "       'got', 'another', 'seen', 'away', 'shit', 'better', 'cool', 'news',\n",
       "       'theyll', 'live', 'type', 'story', 'embryo', 'deextinction',\n",
       "       'watch', 'finally', 'tree', 'fucking', 'whether', 'egg', 'maybe',\n",
       "       'id', 'wont', 'able', '3', 'ever', 'million', 'making', 'part',\n",
       "       'editing', 'heard', 'similar', 'earth', 'there', 'found', 'tusk',\n",
       "       'giant', 'sound', 'used', 'money', 'issue', 'brought', 'trying',\n",
       "       'reviving', 'anyone', 'imagine', 'room', 'give', 'north', 'zoo',\n",
       "       'different', 'anything', 'release', 'working', 'raptor', 'decade',\n",
       "       'forest', 'please', 'start', 'kid', 'instead', 'basically',\n",
       "       'interesting', 'might', 'term', 'place', 'today', 'due', 'without',\n",
       "       '1', 'easy', 'reading', 'bunch', 'die', 'happen', 'birth',\n",
       "       'almost', 'went', 'case', 'using', 'create', 'melting'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This time the list of words is more unique to the subject area\n",
    "# It also takes more words to capture 50% of the total usage\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "counts = Counter(lemmatizer.lemmatize(word) for text in mammoths_df['clean_body'] for word in text if word not in stopwords)\n",
    "mammoth_counts_df = pd.DataFrame(counts.most_common(), columns = ['word','count'])\n",
    "\n",
    "mammoth_counts_df[mammoth_counts_df['count'].cumsum() < (mammoth_counts_df['count'].sum()/2)]['word'].values\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['work', 'time', 'hour', 'home', 'office', 'working', 'day', 'get',\n",
       "       'people', 'im', 'dont', 'like', 'commute', '', 'back', 'job',\n",
       "       'wfh', 'go', 'much', 'also', 'remote', 'even', 'would', 'le',\n",
       "       'commuting', 'article', 'business', 'week', 'make', 'company',\n",
       "       'want', 'done', 'life', 'take', 'way', 'need', 'getting', 'one',\n",
       "       'thing', 'lunch', '2', 'every', 'better', 'ive', 'minute', 'thats',\n",
       "       'going', 'still', 'spend', 'meeting', 'extra', 'year', 'save', 'u',\n",
       "       'really', 'actually', 'money', 'lot', 'traffic', 'think', 'never',\n",
       "       'know', 'insider', 'worker', 'see', 'id', 'around', 'shit', 'big',\n",
       "       'able', 'since', 'fuck', 'mean', 'saved', 'well', 'instead',\n",
       "       'drive', 'could', 'saving', 'youre', '5', 'long', 'good', 'lol',\n",
       "       'everyone', 'yeah', 'doesnt', 'isnt', 'bullshit', 'say',\n",
       "       'something', 'maybe', 'got', 'car', 'walk', 'away', 'there',\n",
       "       'many', 'bos', 'put', 'remotely', 'break', 'come', 'bad', 'feel',\n",
       "       'morning', 'driving', 'gas', 'kid', 'theyre', 'care', 'cant',\n",
       "       'sure', 'point', 'always', 'free', 'pay', 'paid', 'employee',\n",
       "       'end', 'cost', 'worked', 'etc', 'nice', 'space', 'fucking',\n",
       "       'house', 'great', 'used', 'two', 'min', 'per', 'try', 'anything',\n",
       "       'trying', 'desk', 'longer', 'stuff', 'real', 'true', 'look',\n",
       "       'didnt', 'live', 'person', 'hate', 'amount', 'coffee',\n",
       "       'productive', 'help', 'social', 'friend', 'love', 'ill', 'right',\n",
       "       'making', '15', 'family', 'might', 'exactly', 'start', 'sometimes',\n",
       "       'let', 'ever', 'eat', 'use', 'bit', '3', '10', 'probably'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This time the list of words is more unique to the subject area\n",
    "# It also takes more words to capture 50% of the total usage\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "counts = Counter(lemmatizer.lemmatize(word) for text in remote_df['clean_body'] for word in text if word not in stopwords)\n",
    "remote_counts_df = pd.DataFrame(counts.most_common(), columns = ['word','count'])\n",
    "\n",
    "remote_counts_df[remote_counts_df['count'].cumsum() < (remote_counts_df['count'].sum()/2)]['word'].values\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalised Counts\n",
    "\n",
    "Unsurprisingly, the length of a piece of text changes how many different words can appear in it. This is a complex point, because the change is both the direct mathematical one, but also a change in the context and thus in the words the author might choose. Short texts, like on social media, use words differently than long essays. Since our texts share a similar context, we will normalise by the size of the corpus to produce comparable word frequencies between the two.\n",
    "\n",
    "You could perform this step either at the level of the corpus, or on each piece of text. We will start with the two posts as our level of aggregation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert-info alert\">\n",
    "\n",
    "**Exercise 4.1:** Use the lemmatized word counts to create normalised frequencies for each post\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "mammoth_counts_df['frequency'] = mammoth_counts_df['count']/mammoth_counts_df['count'].sum()\n",
    "remote_counts_df['frequency'] = remote_counts_df['count']/remote_counts_df['count'].sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency of `work` in mammoths: 0.0015916526660182157\n",
      "Frequency of `work` in remote work: 0.03202410176299933\n"
     ]
    }
   ],
   "source": [
    "# now we can compare how frequently we see the word `work`\n",
    "# it is 20x more frequent in the remote work article!\n",
    "\n",
    "print(f\"Frequency of `work` in mammoths: {mammoth_counts_df[mammoth_counts_df['word'] == 'work']['frequency'].values[0]}\")\n",
    "print(f\"Frequency of `work` in remote work: {remote_counts_df[remote_counts_df['word'] == 'work']['frequency'].values[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert-info alert\">\n",
    "\n",
    "**Exercise 4.2:** Make a function which returns the frequency of a word in each comment section, or zero if it does not appear.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm setting the index of the dfs to the words to make it easier to use them for lookups\n",
    "\n",
    "mammoth_counts_df = mammoth_counts_df.set_index('word')\n",
    "remote_counts_df = remote_counts_df.set_index('word')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq_lookup(word):\n",
    "    if word in mammoth_counts_df.index:\n",
    "        mf = mammoth_counts_df.loc[word,'frequency']\n",
    "    else: \n",
    "        mf = 0\n",
    "    if word in remote_counts_df.index:\n",
    "        rf = remote_counts_df.loc[word, 'frequency']\n",
    "    else:\n",
    "        rf = 0\n",
    "    return round(mf,5), round(rf,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.02538, 0)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_lookup('mammoth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and|(0, 0)\n",
      "nothing|(0.00133, 0.00106)\n",
      "beats|(0, 0)\n",
      "working|(0.00088, 0.01216)\n",
      "in|(0, 6e-05)\n",
      "your|(0, 0)\n",
      "pjs|(0, 0.00033)\n"
     ]
    }
   ],
   "source": [
    "# Now, if we find an example post, we can guess which article it came from:\n",
    "\n",
    "# most of these words have similar frequencies in both, but `commuting` is far more frequent in remote work\n",
    "# therefore, we think this is a remote work post (of course we know it is)\n",
    "\n",
    "post = remote_df['clean_body'][100]\n",
    "\n",
    "for word in post:\n",
    "    print(word + '|' +str(freq_lookup(word)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment\n",
    "\n",
    "Finally, we will do a very basic pass at sentiment analysis. As usual, you need to be careful that the tool you use is suited for the context. NLTK has a simple pre-trained sentiment analyzer which gives scores for short pieces of text like reddit comments. We will use it to demonstrate the idea. If you want to perform sentiment analysis in your summative or thesis, I would recommend looking into `Hugging Face` for a better suited pre-trained model.\n",
    "\n",
    "Because the relationship between words matters more here, we do not want to remove stopwords or stem/lemmatize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert-info alert\">\n",
    "\n",
    "**Exercise 5.1:** Use the sentiment analyzer on the example sentence\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "example_words = 'mammoths do not like working remotely because they prefer social activities'\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "sia.polarity_scores(example_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see here that 82.6% of the sentence was neutral words, but 17.4% of the words were negative in mood. The model does a custom combination to conclude that this is a moderately negative sentence. (Seems right to me)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert-info alert\">\n",
    "\n",
    "**Exercise 5.2:** Use the sentiment analyzer on the raw body text in the databases to add a new `sentiment` column\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mammoths_df['sentiment'] = mammoths_df['body'].apply(lambda x: sia.polarity_scores(x)['compound'])\n",
    "remote_df['sentiment'] = remote_df['body'].apply(lambda x: sia.polarity_scores(x)['compound'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert-info alert\">\n",
    "\n",
    "**Exercise 5.3:** Find a positive and negative comment from each post and see how well the analyzer worked \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the most negative mammoths post - it does seem very negative\n",
    "mammoths_df[mammoths_df['sentiment'] == mammoths_df['sentiment'].min()]['body'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the most positive mammoths post - it has a large quote from the news article which may be misleading\n",
    "mammoths_df[mammoths_df['sentiment'] == mammoths_df['sentiment'].max()]['body'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the most negative remote work post\n",
    "remote_df[remote_df['sentiment'] == remote_df['sentiment'].min()]['body'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the most positive remote work post\n",
    "remote_df[remote_df['sentiment'] == remote_df['sentiment'].max()]['body'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking a look at the distribution, the mammoth comments have plenty of positive and negative\n",
    "mammoths_df[mammoths_df['sentiment']!=0]['sentiment'].hist(density=True)\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the remote work comments have a more positive skew \n",
    "\n",
    "remote_df[remote_df['sentiment']!=0]['sentiment'].hist(density=True)\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save your work\n",
    "\n",
    "We'll use this data again next week for visualizations. Save each of the following dfs to a file:\n",
    "\n",
    "- mammoth_counts_df\n",
    "- remote_counts_df\n",
    "- mammoths_df\n",
    "- remote_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mammoth_counts_df.to_csv('mammoths_counts.csv')\n",
    "remote_counts_df.to_csv('remote_counts.csv')\n",
    "mammoths_df.to_csv('mammoths.csv',index=False)\n",
    "remote_df.to_csv('remote_work.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Homework: Naive Bayes Classifier (Optional)\n",
    "\n",
    "A Naive Bayes classifier is a basic NLP technique which lets you classify a text based on previously established categories. It assumes that each word in a text is drawn at random from a bag of possible words, where the bag is different depending on the category. (Like what we did above, where `commuting` is more common in the remote work category.) \n",
    "\n",
    "Using these probabilities, the total probability of the text belonging to each category is just the combined probabilities of each word appearing if the text was from that category, so we multiply the probabilities together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq_ratio(word):\n",
    "    smoothing_prb = 1/(1/mammoth_counts_df['frequency'].min() + 1/remote_counts_df['frequency'].min())\n",
    "    if word in mammoth_counts_df.index:\n",
    "        mf = mammoth_counts_df.loc[word,'frequency']\n",
    "    else: \n",
    "        mf = smoothing_prb\n",
    "    if word in remote_counts_df.index:\n",
    "        rf = remote_counts_df.loc[word, 'frequency']\n",
    "    else:\n",
    "        rf = smoothing_prb\n",
    "    return mf/rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes(text):\n",
    "    freqs = 1\n",
    "    for word in text:\n",
    "        freqs = freqs * freq_ratio(word) \n",
    "    return freqs/(1+freqs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = naive_bayes(mammoths_df['clean_body'].iloc[0])\n",
    "print(f'Prob of mammoths: {pred}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1 = mammoths_df['clean_body'].apply(naive_bayes)\n",
    "pred2 = remote_df['clean_body'].apply(naive_bayes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, our classifier is pretty good! It was only wrong on 78 out of the ~1800 comments. However, there are a lot of things you would need to be more careful about in a real classification task. Most notably, we are testing the final accuracy on the training data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Correctly predicted mammoth: {len(pred1[pred1>.5])}\")\n",
    "print(f\"Incorrectly predicted remote work: {len(pred1[pred1<=.5])}\")\n",
    "print(f\"Incorrectly predicted mammoth: {len(pred2[pred2>.5])}\")\n",
    "print(f\"Corrently predicted remote work: {len(pred2[pred2<=.5])}\")"
   ]
  }
 ],
 "metadata": {
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_app_layout": "article",
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "e14e0833-7f70-4764-8920-14303e12f9cc",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
